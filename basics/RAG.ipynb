{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d73fd9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7429df81",
   "metadata": {},
   "source": [
    "WebBaseLoader는 WEB의 URL을 주면, 페이지를 가져오는 역할을 함\n",
    "- 외부에서 가져오는 문서여서, RAG라는 형식으로 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10f10e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d9976b",
   "metadata": {},
   "source": [
    "### <font color=green>llama를 쓰는 경우 아래와 같이 패키지 설치해줘야함</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc52c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bitsandbytes==0.40.0 einops==0.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a023efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cafc8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import cuda, bfloat16\n",
    "# import transformers\n",
    "\n",
    "# #model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "# model_id = 'meta-llama/Meta-Llama-3-8B'\n",
    "\n",
    "# device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# # set quantization configuration to load large model with less GPU memory\n",
    "# # this requires the `bitsandbytes` library\n",
    "# bnb_config = transformers.BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type='nf4',\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_compute_dtype=bfloat16\n",
    "# )\n",
    "\n",
    "# # begin initializing HF items, you need an access token\n",
    "# hf_auth = '[HF_AUTH]'\n",
    "# model_config = transformers.AutoConfig.from_pretrained(\n",
    "#     model_id,\n",
    "#     use_auth_token=hf_auth\n",
    "# )\n",
    "\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     trust_remote_code=True,\n",
    "#     config=model_config,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map='auto',\n",
    "#     use_auth_token=hf_auth\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05f0016",
   "metadata": {},
   "source": [
    "### <font color=yellow>1. indexing: Load</font>\n",
    "- DocumentLoaders를 사용하여 블로그 내용을 로드\n",
    "- Documents는 소스에서 데이터를 로드하고 Documents 목록을 반환하는 개체\n",
    "- Document는 page_content(str)와 메타데이터(dict)를 포함하는 개체\n",
    "- urllib을 사용하여, 웹 URL에서 HTML을 로드하고 BeautifulSoup을 사용하여 텍스트로 구문 분석함\n",
    "    - bs_kwags를 통해 BeautifulSoup 파서에 매개변수를 전달하여 HTML -> 텍스트 구문 분석을 사용자 정희할 수 있음\n",
    "    - (BeautifulSoup 문서 참조)\n",
    "    - 이 경우 클래스 \"post-content\", \"post-title\" 또는 \"post-header\"가 있는 HTML 태그만 관련되므로 다른 모든 것을 제거함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846d5863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "925618b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "len(docs[0].page_content)\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed5c5d7",
   "metadata": {},
   "source": [
    "### <font color=yellow>2. indexing: Split</font>\n",
    "- 로드된 문의 길이는 42,000자를 넘음\n",
    "- 이는 너무 길어서 많은 모델의 컨텍스트 창에 맞지 않음 \n",
    "- 컨텍스트 창에 전체 게시물을 맞출 수 있는 모델의 경우에도 모델은 매우 긴 입력에서 정보를 찾는 데 어려움을 겪을 수 있음\n",
    "\n",
    "- Document를 처리하기 위해 임베딩 및 벡터 저장을 위해 청크로 분할\n",
    "- 이는 런타임 시 블로그 게시물의 가장 관련성이 높은 부분만 검색하는 데 도움이 됨\n",
    "\n",
    "- 이 경우 문서를 1000자의 청크로 분할하고 청크 사이에 200자가 겹치도록 하겠음 \n",
    "- 중복은 진술과 관련된 중요한 맥락에서 진술을 분리할 가능성을 완화하는 데 도움이 됨\n",
    "- 우리는 각 청크가 적절한 크기가 될 때까지 새 줄과 같은 공통 구분 기호를 사용하여 문서를 재귀적으로 분할하는 RecursiveCharacterTextSplitter를 사용함 \n",
    "- 이는 일반적인 텍스트 사용 사례에 권장되는 텍스트 분할기임\n",
    "\n",
    "- add_start_index=True로 Document 내에서 각 분할 Document가 시작되는 문자 인덱스가 메타데이터 속성 \"start_index\"로 유지되도록 설정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd47105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT는 4000 tokens 정도됨 (Palcon이 8000 tokens 정도)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a941e83",
   "metadata": {},
   "source": [
    "데이터 splitter를 이용해 블록화를 시키면, 잘린 블록마다 메타데이터를 가지게 됨\n",
    "- 긴 문서중에, 내가 찾은 이 블록이 몇 번째 블록이고, 어디서 부터 시작하고, 이런 소스 데이터를 가지게됨\n",
    "- 사용자 검색이 들어오면, 유사도 측정을 하고, 그것이 답이다라고 답변을 줄때, 이 메타데이터를 함께 줄 수 있음\n",
    "- 내가 지금 만든 이 문장이 어느 부분에 어느 포인트를 가지고 이렇게 문장을 만들었어 라고 인지하기 위해 메타데이터를 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bb81980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " 'start_index': 8436}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits)\n",
    "len(all_splits[0].page_content)\n",
    "all_splits[10].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbdfb6f",
   "metadata": {},
   "source": [
    "### <font color=yellow>3. indexing: Store</font>\n",
    "- 이제 런타임에 검색할 수 있도록 667개의 텍스트 청크를 인덱싱해야 함 \n",
    "- 이를 수행하는 가장 일반적인 방법은 분할된 각 문서의 내용을 포함하고 이러한 포함을 벡터 데이터베이스(또는 벡터 저장소)에 삽입함\n",
    "- 분할을 검색하려는 경우 텍스트 검색 쿼리를 가져와 이를 포함하고 일정의 \"유사성\" 검색을 수행하여 쿼리 포함과 가장 유사한 임베딩이 있는 저장된 분할을 식별함 \n",
    "- 가장 간단한 유사성 측정은 코사인 유사성\n",
    "    - 즉, 각 임베딩 쌍(고차원 벡터) 사이의 각도의 코사인을 측정함\n",
    "\n",
    "- Chroma 벡터 저장소와 OpenAIEmbeddings 모델을 사용하여 단일 명령에 모든 문서 분할을 포함하고 저장할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dfc0e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())         #\n",
    "#vectorstore = Chroma.from_documents(documents=all_splits, embedding=FastEmbedEmbeddings())     # FastEmbedEmbeddings 오류로 OpenAIEmbeddings로 수정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8512bce4",
   "metadata": {},
   "source": [
    "### <font color=yellow>4. Retrieval and Generation Retrieve</font>\n",
    "- 이제 실제 애플리케이션 로직을 작성하곘음 \n",
    "- 우리는 사용자 질문을 받아 해당 질문과 관련된 문서를 검색하고 검색된 문서와 초기 질문을 모델에 전달하고 답변을 반환하는 간단한 애플리케이션을 만들고 싶음\n",
    "\n",
    "- 먼저 문서 검색을 위한 논리를 정의해야 함 \n",
    "- LangChain은 문자열 쿼리가 주어지면 관련 문서를 반환할 수 있는 인덱스를 래핑하는 Retriever 인터페이스를 정의햐여험\n",
    "\n",
    "- 리트리버의 가장 일반적인 유형은 벡터 스토어의 유사성 검색 기능을 사용하여 검색을 용이하게 하는 벡터 스토어 리트리버임 - VectorStore.as_retriever()를 사용하면 벡터 스토어를 쉽게 리트리버로 변환할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4437120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")\n",
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d62f51fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1256bb",
   "metadata": {},
   "source": [
    "### <font color=yellow>5. Retrieval and Generation: Generate</font>\n",
    "- 질문을 받고, 관련 문서를 검색하고, 프롬프트를 구성하고, 이를 모델에 전달\n",
    "- 출력을 구문 분석하는 체인에 모두 함께 넣겠음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d331a83e",
   "metadata": {},
   "source": [
    "<font color=green>rlm/rag-prompt를 가져다가 프롬프트 형식으로 사용할 것임</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8e43372",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jjsd4\\workspace\\torch\\Lib\\site-packages\\langsmith\\client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "example_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83370f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: filler question \n",
      "Context: filler context \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77a3fecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition involves breaking down a complex task into smaller, more manageable steps to enhance model performance. Techniques like Chain of Thought and Tree of Thoughts use this method to interpret the model’s thinking process and explore multiple reasoning possibilities. It can be achieved through simple prompts, task-specific instructions, or human inputs.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vector_store = Chroma.from_documents(documents=chunks, embedding=FastEmbedEmbeddings())\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\n",
    "        \"k\": 3,\n",
    "        \"score_threshold\": 0.5,\n",
    "    },\n",
    ")\n",
    "\n",
    "chain = ({\"context\":retriever, \"question\": RunnablePassthrough()}\n",
    "              | prompt\n",
    "              | llm\n",
    "              | StrOutputParser())\n",
    "\n",
    "chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a759f9d",
   "metadata": {},
   "source": [
    "## <font color=yellow>나중에 LLaMA 3.1 8B로 모델만 바꿔서 해보기</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
